{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suchizz/doc_analyzer_frontend/blob/main/chatbot_theme_identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t4WiVuFrIkLF"
      },
      "outputs": [],
      "source": [
        "# âœ… Install required packages\n",
        "!pip install -q fastapi uvicorn nest_asyncio pyngrok\n",
        "!pip install -q langchain sentence-transformers faiss-cpu PyMuPDF transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kmaGVv46JeVY"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gb-Md_pnIyNC"
      },
      "outputs": [],
      "source": [
        "# âœ… Imports\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b1cLheATNkJ",
        "outputId": "1b2ddf5e-49e9-4c7d-dff8-55ab11a379c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# âœ… Initialize summarization model\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "summarizer = PegasusForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from fastapi.responses import JSONResponse\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "import tempfile\n",
        "import os\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRCs9kYvDw4Q",
        "outputId": "cf11afc1-47f1-4444-8ee4-f469e1707db2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FPkJEMSXFoXE"
      },
      "outputs": [],
      "source": [
        "app = FastAPI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/analyze\")\n",
        "async def analyze(files: list[UploadFile] = File(...), question: str = Form(...)):\n",
        "    all_chunks = []\n",
        "    file_metadata = []\n",
        "\n",
        "    for file in files:\n",
        "        filename = file.filename\n",
        "        temp_path = os.path.join(tempfile.gettempdir(), filename)\n",
        "        with open(temp_path, \"wb\") as f:\n",
        "            f.write(await file.read())\n",
        "\n",
        "        pdf = fitz.open(temp_path)\n",
        "        for page_num, page in enumerate(pdf, start=1):\n",
        "            paragraphs = page.get_text(\"blocks\")\n",
        "            for para_idx, (x0, y0, x1, y1, text, block_no, block_type) in enumerate(paragraphs):\n",
        "                sentences = sent_tokenize(text.strip())\n",
        "                for sent_idx, sentence in enumerate(sentences, start=1):\n",
        "                    meta = {\n",
        "                        \"doc_name\": filename,\n",
        "                        \"page\": page_num,\n",
        "                        \"paragraph\": para_idx + 1,\n",
        "                        \"sentence\": sent_idx\n",
        "                    }\n",
        "                    if len(sentence.split()) > 5:\n",
        "                        all_chunks.append(Document(page_content=sentence, metadata=meta))\n",
        "\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    db = FAISS.from_documents(all_chunks, embedder)\n",
        "    results = db.similarity_search(question, k=6)\n",
        "\n",
        "    answer_snippets = []\n",
        "    doc_table = []\n",
        "    theme_map = {}\n",
        "\n",
        "    for doc in results:\n",
        "        meta = doc.metadata\n",
        "        citation = f\"{meta['doc_name']} â€“ Page {meta['page']}, Para {meta['paragraph']}, Sent {meta['sentence']}\"\n",
        "        link = f\"[{citation}](#jump-to-{meta['doc_name']}-{meta['page']})\"\n",
        "        snippet = f\"ðŸ”¹ {doc.page_content}  \\nðŸ“Œ {link}\"\n",
        "        answer_snippets.append(snippet)\n",
        "\n",
        "        doc_table.append({\n",
        "            \"document\": meta['doc_name'],\n",
        "            \"page\": meta['page'],\n",
        "            \"paragraph\": meta['paragraph'],\n",
        "            \"sentence\": meta['sentence'],\n",
        "            \"answer\": doc.page_content\n",
        "        })\n",
        "\n",
        "        # Basic rule-based theme clustering\n",
        "        if \"graduation\" in doc.page_content.lower():\n",
        "            theme_map.setdefault(\"Education Impact\", []).append((meta['doc_name'], doc.page_content))\n",
        "        elif \"job\" in doc.page_content.lower():\n",
        "            theme_map.setdefault(\"Career Loss\", []).append((meta['doc_name'], doc.page_content))\n",
        "        else:\n",
        "            theme_map.setdefault(\"General Insight\", []).append((meta['doc_name'], doc.page_content))\n",
        "\n",
        "    theme_output = \"Synthesized (theme) answer (chat format):\\n\"\n",
        "    for idx, (theme, items) in enumerate(theme_map.items(), 1):\n",
        "        theme_output += f\"\\nTheme {idx} â€“ {theme}:\\n\"\n",
        "        added = set()\n",
        "        for doc_id, content in items:\n",
        "            if (doc_id, content) not in added:\n",
        "                theme_output += f\"{doc_id}: {content[:120]}...\\n\"\n",
        "                added.add((doc_id, content))\n",
        "\n",
        "    return JSONResponse({\n",
        "        \"question\": question,\n",
        "        \"direct_answers\": answer_snippets,\n",
        "        \"documents\": doc_table,\n",
        "        \"theme_summary\": theme_output\n",
        "    })\n"
      ],
      "metadata": {
        "id": "jSVCukMjETPM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oTlYhveLQSa",
        "outputId": "f1d603b7-d8d1-4e28-f3c8-2cd323170aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Your backend is live at: NgrokTunnel: \"https://6dd3-34-90-102-224.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# âœ… Launch ngrok tunnel\n",
        "# Set your ngrok authtoken here. Replace \"YOUR_AUTHTOKEN\" with your actual authtoken.\n",
        "# You can find your authtoken at https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "try:\n",
        "    ngrok.set_auth_token(\"2yQJk5RM2Fe1imsdbwyHk3nsJwF_7G9A5xE146vU8cXVc7UYk\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"ðŸ”— Your backend is live at: {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while starting ngrok: {e}\")\n",
        "    print(\"Please ensure you have set your ngrok authtoken correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f2ye9kSxru1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WygFk-9WbbU",
        "outputId": "43a67152-c8cc-448f-b1b1-7bbef1dc3e89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [179]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-15' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     35.203.151.101:0 - \"POST /analyze HTTP/1.1\" 200 OK\n",
            "INFO:     35.203.151.101:0 - \"POST /analyze HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# âœ… Start FastAPI server\n",
        "# âœ… Start FastAPI server\n",
        "# Apply nest_asyncio patch to allow running asyncio in a Jupyter notebook\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyNIP+tyE5rz5uAl9LZ7TbhW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}